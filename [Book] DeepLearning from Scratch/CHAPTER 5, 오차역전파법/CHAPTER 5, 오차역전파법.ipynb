{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13125 0.1317\n",
      "0.9034833333333333 0.9069\n",
      "0.9229 0.9237\n",
      "0.9338333333333333 0.9339\n",
      "0.9444666666666667 0.9437\n",
      "0.9495333333333333 0.9475\n",
      "0.9568666666666666 0.9563\n",
      "0.9613666666666667 0.958\n",
      "0.96315 0.9587\n",
      "0.9685333333333334 0.9652\n",
      "0.97055 0.9657\n",
      "0.97125 0.9666\n",
      "0.9729833333333333 0.9677\n",
      "0.9733666666666667 0.9659\n",
      "0.9762333333333333 0.9683\n",
      "0.9769833333333333 0.97\n",
      "0.9776833333333333 0.9685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nx_batch = x_train[:3]\\nt_batch = t_train[:3]\\n\\ngrad_numerical = network.numerical_gradient(x_batch, t_batch)\\ngrad_backprop = network.gradient(x_batch, t_batch)\\n\\n# 각 가중치 차이의 절댓값을 구한 후, 그 절댓값들의 평균을 낸다.\\nfor key in grad_numerical.keys():\\n    # 수치 미분과 오차역전파법의 결과 오차가 0이 되는 일은 드물다, 즉 값이 작아질 수록 좋다.\\n    diff = np.average(np.abs(grad_backprop[key] - grad.numerical[key]))\\n    print(key + \":\" + diff)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from dataset.mnist import load_mnist\n",
    "from common.gradient import numerical_gradient\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "\n",
    "# 곱셈 계층\n",
    "class MulLayer:\n",
    "    def __init(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    # 순전파 \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        return x * y\n",
    "    \n",
    "    # 역전파, 상류에서 넘어온 미분에 순전파 때의 입력과 출력값을 바뀌서 곱한뒤 하류로 흘린다.\n",
    "    def backward(self, dout): # dout = 상류에서 넘어온 미분값, 맨 처음엔 순전파 때의 결과값의 미분값\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return  dx, dy\n",
    "    \n",
    "# 덧셈 계층\n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass # 아무런 실행 없이 넘김\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        return x + y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        \n",
    "        return dx, dy\n",
    "    \n",
    "# ReLU 계층(활성화 함수)\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = x <= 0 # 인수 X(넘파이 배열)의 원소를 하나씩 조건에 맞게 True나 False로 변환\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0 # 인덱스 값으로 True가 들어가면 해당값 추가, False가 들어가면 해당 값은 뺀다.\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "# Sigmoid 계층(활성화 함수)\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out # 순전파의 출력을 인스턴스 변수 out(self.out)에 저장했다가 역전파 계산 때 그 값을 사용한다.\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "# Affine 계층(합성곱 계층), 기하학에서 신경망의 순전파 때 수행하는 행렬의 곱을 의미한다.\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T) # numpy 배열에 .T 속성을 주면 전치연산처리가 되어서 원래 배열의 행과 열이 바뀐다\n",
    "        self.dW = np.dot(self.x.T, dout) # 행렬의 곱 계산\n",
    "        self.db = np.sum(dout, axis=0) # 각 행렬의 원소를 대치되는 인덱스를 가진 원소끼리 더한다.\n",
    "        \n",
    "        return dx\n",
    "\n",
    "# Softmax-with-Loss 계층, 입력 값을 정규화하여 출력한다.\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실\n",
    "        self.y = None # softmax의 출력\n",
    "        self.x = None # 정답 레이블(one-hot 벡터)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        # 계층 생성, 2층\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu() # 활성화 계층(함수)\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x) # 각 계층의 순전파 실행\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return self.lastLayer.forward(y ,t) # softmax의 손실함수 구하기\n",
    "    \n",
    "    # 정확도(예측값(출력값)과 정답 사이의 수치) 계산\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "            \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x ,t):\n",
    "        # 순전파\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # 역전파\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout) # 각 계층의 역전파 실행\n",
    "            \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        return grads\n",
    "    \n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10) # 신경망 생성\n",
    "\n",
    "iters_num = 10000 # 전체 입력 수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1) # 에폭의 단위\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 훈련 모델 중 배치 수 만큼 무작위 추출\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    # batch_mask 에서 무작위 추출된 값들이 인덱스가 되고 그 해당 인덱스에 맞는 값을 제외하곤 나머진 0이 됨\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 오차역전파법으로 기울기를 구한다.\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key] # 오차역전파법으로 기울기 구하면 각 매개변수에 구한 기울기 X 학습률을 빼서 갱신\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1 에폭 만큼 반복했을 시\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)\n",
    "\n",
    "\"\"\"\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 각 가중치 차이의 절댓값을 구한 후, 그 절댓값들의 평균을 낸다.\n",
    "for key in grad_numerical.keys():\n",
    "    # 수치 미분과 오차역전파법의 결과 오차가 0이 되는 일은 드물다, 즉 값이 작아질 수록 좋다.\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad.numerical[key]))\n",
    "    print(key + \":\" + diff)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
