1. GPU를 활용한 고속화
GPU는 병령 수치 연산을 고속으로 처리할 수 있다. 예를들어 대량의 단일 곱셈-누산
(또는 큰 행렬의 곱)같은 걸 수행할 때 이런 연산은 GPU의 특기이다.
대부분의 딥러닝 프레임워크는 "엔비디아" GPU에서만 해택을 받을 수 있다.
엔비디아의 GPU 컴퓨팅용 통합 개발 환경인 "CUDA"를 사용하기 때문이다.
예시로 CUDA 위에서 작동하는 "cuDNN" 이라는 라이브러리가 있다.

* 반대로 CPU는 연속적인 복잡한 연산을 잘 처리한다.
* CPU = 어려운 단일 계산 처리 / GPU = 쉬운 여러 계산 한번에 처리

2. 분산 학습
다수의 CPU와 기기로 계산을 분산하여 학습을 진행하는 것

* CPU 수와 비례해서 학습 속도가 빨라진다.

3. 연산 정밀도와 비트 줄이기
메모리 용량 면에서 대향의 가중치 매개변수와 중간 데이터를 메모리에 저장해야 한다는
것과 버스 대역폭 면에서 GPU(혹은 CPU)의 버슬르 흐르는 데이터가 많아지는 것은
딥러닝 고속화에 병목이 될 수 있다. 이러한 경우를 고려하면 네트워크로 주고받는 데이터의
"비트수는 최소"로 만드는 것이 바람직하다.

* 64bit double-precision, 32bit single-precision, 16bit half-precision 이 대표적이다
ㄴ 비트 수가 작아질 수록 학습 고속화에 유리하다
ㄴ 2016년도 기준, 엔비디아의 GPU의 파스칼 아키텍처부터 16bit half-precision 지원해왔다.
* 임베디드용으로 딥러닝 모델을 만들 때 특히 중요한 분야이다.