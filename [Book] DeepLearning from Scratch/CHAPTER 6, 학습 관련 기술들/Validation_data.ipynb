{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "하이퍼파라미터에는 각 층의 뉴런 수, 배치 크기, 매개변수 갱신 시의 학습률, 가중치 감소 등이 있다.\n",
    "이러한 하이퍼파라미터 값은 수동적으로 적절히 설정하지 않으면 모델의 성능이 크게 떨어지기도 한다.\n",
    "\n",
    "하이퍼파라미터의 성능을 평가할 때는 시험 데이터가 아닌 반드시 검증 데이터(Validation data)를 이용해야 한다.\n",
    "시험 데이터를 사용하여 하이퍼파라미터를 조정하면 하이퍼파라미터 값이 시험 데이터에 오버피팅되고 이로 인해 범용성이 떨어지기 때문이다.\n",
    "훈련 데이터는 매개변수 학습에, 검증 데이터는 하이퍼파라미터 성능 평가에 마지막으로 시험 데이터는 신경망의 범용 성능 평가에 사용된다.\n",
    "검증 데이터를 얻는 가장 간단한 방법은 훈련 데이터 중 20% 정도를 검증 데이터로 먼저 분리하는 것이다.\n",
    "\"\"\"\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist()\n",
    "x_train, t_train = shuffle_dataset(x_train, t_train) # 훈련 데이터를 섞는다. 데이터 셋 안의 데이터가 치우쳐 있을 수도 있기 때문이다.\n",
    "\n",
    "validation_rate = 0.20\n",
    "validation_num = int(x_train.shape[0] * validaion_rate)\n",
    "\n",
    "x_val = x_train[:validation_num]\n",
    "t_val = t_train[:validation_num]\n",
    "x_train = x_train[:validation_num]\n",
    "t_train = t_train[:validation_num]\n",
    "\n",
    "\"\"\"\n",
    "하이퍼파라미터를 최적화할 때의 핵심은 하이퍼파라미터의 '최적 값'이 존재하는 범위를 조금씩 줄여간다는 것이다.\n",
    "우선 대략적인 범위를 설정하고 그 범위에서 무작위로 하이퍼파라미터 값을 골라낸 후(무작위 샘플링), 그 값으로 정확도를 평가한다.\n",
    "이 작업을 특정 횟수만큼 반복한 뒤 정확도를 확인하면서 하이퍼파라미터의 최적 값의 범위를 좁혀가는 것이다.\n",
    "\n",
    "하이퍼파라미터의 범위는 대략적으로 지정하는 것이 효과적이다. 이를 '로그 스케일(log scale)로 지정'한다고 한다.\n",
    "하이퍼파라미터 최적화 시에는 기간이 매우 오래 걸린다.(최소 며칠)\n",
    "어느 정도 범위가 좁아지면 그 범위에서 값을 하나 골라낸다. 이것이 하이퍼파라미터 최적화 하는 하나의 방법이다.\n",
    "세련된 기법을 원한다면 베이즈 최적화(Bayesian optimization)을 이용하면 된다.\n",
    "\n",
    "* 로그 스케일(log scale) 범위 : 0.001~1000 사이 같은 범위\n",
    "* 베이즈 최적화(Bayesian optimization) : 베이즈 정리를 중심으로 한 수학 이론을 구사하여 더 엄밀하고 효율적으로 최적화를 수행한다.\n",
    "\"\"\"\n",
    "\n",
    "# numpy(np)의 random.uniform(low, high, size)는, low~high 사이의 균일한 분포의 무작위 원소가 size 만큼 있는(default값=1)값을 반환한다.\n",
    "# size 가 2 이상인 경우 값들이 배열에 저장되어 반환된다.\n",
    "weight_dcay = 10 ** np.random.uniform(-8, -4) # 가중치 감소치\n",
    "lr = 10 ** np.random.uniform(-6, -2) # 학습률"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
